<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-physical-ai/sensor-systems" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Sensor Systems: Perceiving the World | AI Native Development</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://bilalmk.github.io/ai-native-book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://bilalmk.github.io/ai-native-book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://bilalmk.github.io/ai-native-book/docs/physical-ai/sensor-systems"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Sensor Systems: Perceiving the World | AI Native Development"><meta data-rh="true" name="description" content="A robot&#x27;s ability to perform tasks in the physical world is fundamentally limited by its ability to perceive its environment and its own state. Sensors provide the raw data that perception algorithms use to build a coherent world model. This section introduces the core sensor technologies that enable modern robotics."><meta data-rh="true" property="og:description" content="A robot&#x27;s ability to perform tasks in the physical world is fundamentally limited by its ability to perceive its environment and its own state. Sensors provide the raw data that perception algorithms use to build a coherent world model. This section introduces the core sensor technologies that enable modern robotics."><link data-rh="true" rel="icon" href="/ai-native-book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://bilalmk.github.io/ai-native-book/docs/physical-ai/sensor-systems"><link data-rh="true" rel="alternate" href="https://bilalmk.github.io/ai-native-book/docs/physical-ai/sensor-systems" hreflang="en"><link data-rh="true" rel="alternate" href="https://bilalmk.github.io/ai-native-book/docs/physical-ai/sensor-systems" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Physical AI","item":"https://bilalmk.github.io/ai-native-book/docs/physical-ai/"},{"@type":"ListItem","position":2,"name":"Sensor Systems","item":"https://bilalmk.github.io/ai-native-book/docs/physical-ai/sensor-systems"}]}</script><link rel="alternate" type="application/rss+xml" href="/ai-native-book/blog/rss.xml" title="AI Native Development RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/ai-native-book/blog/atom.xml" title="AI Native Development Atom Feed"><link rel="stylesheet" href="/ai-native-book/assets/css/styles.0c55421b.css">
<script src="/ai-native-book/assets/js/runtime~main.30308d2f.js" defer="defer"></script>
<script src="/ai-native-book/assets/js/main.9a3b3c8e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><a class="navbar__brand" href="/ai-native-book/"><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai-native-book/docs/physical-ai"><span title="Physical AI" class="categoryLinkLabel_W154">Physical AI</span></a><button aria-label="Collapse sidebar category &#x27;Physical AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/physical-ai/physical-ai-introduction"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/physical-ai/foundations-of-physical-ai"><span title="Foundations of Physical AI" class="linkLabel_WmDU">Foundations of Physical AI</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/physical-ai/from-digital-ai-to-robots"><span title="From Digital AI to Robots That Understand Physical Laws" class="linkLabel_WmDU">From Digital AI to Robots That Understand Physical Laws</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/physical-ai/humanoid-robotics-landscape"><span title="Humanoid Robotics Landscape" class="linkLabel_WmDU">Humanoid Robotics Landscape</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-native-book/docs/physical-ai/sensor-systems"><span title="Sensor Systems" class="linkLabel_WmDU">Sensor Systems</span></a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai-native-book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-native-book/docs/physical-ai"><span>Physical AI</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Sensor Systems</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Sensor Systems: Perceiving the World</h1></header>
<p>A robot&#x27;s ability to perform tasks in the physical world is fundamentally limited by its ability to perceive its environment and its own state. Sensors provide the raw data that perception algorithms use to build a coherent world model. This section introduces the core sensor technologies that enable modern robotics.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning Objectives" title="Direct link to Learning Objectives" translate="no">​</a></h3>
<p>After completing this section, you will be able to:</p>
<ul>
<li class="">Understand the function of key sensor modalities in robotics.</li>
<li class="">Differentiate between proprioceptive and exteroceptive sensors.</li>
<li class="">Explain the high-level principles of LiDAR, cameras, IMUs, and force/torque sensors.</li>
<li class="">Recognize the primary applications and conceptual limitations of each sensor type.</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-role-of-perception">The Role of Perception<a href="#the-role-of-perception" class="hash-link" aria-label="Direct link to The Role of Perception" title="Direct link to The Role of Perception" translate="no">​</a></h2>
<p>Perception is the process of turning raw sensor data into a meaningful understanding of the robot&#x27;s state and its environment. We can broadly categorize sensors into two main types:</p>
<ul>
<li class=""><strong>Exteroceptive Sensors</strong>: These sensors measure properties of the external world. They are the robot&#x27;s &quot;senses&quot; and include technologies like cameras and LiDAR that perceive objects, distances, and surfaces.</li>
<li class=""><strong>Proprioceptive Sensors</strong>: These sensors measure the internal state of the robot itself. Examples include motor encoders that track joint angles and IMUs that measure orientation and acceleration.</li>
</ul>
<p>A robust robotic system integrates data from both types to build a comprehensive model for decision-making.</p>
<p>[Diagram: A humanoid robot with labels pointing to different sensors. Exteroceptive sensors like cameras in the head and LiDAR on the torso are contrasted with proprioceptive sensors like joint encoders in the limbs.]</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="lidar-light-detection-and-ranging">LiDAR (Light Detection and Ranging)<a href="#lidar-light-detection-and-ranging" class="hash-link" aria-label="Direct link to LiDAR (Light Detection and Ranging)" title="Direct link to LiDAR (Light Detection and Ranging)" translate="no">​</a></h2>
<p>LiDAR is an active sensing method that illuminates the environment with laser beams and measures the reflected light. It is a cornerstone technology for tasks requiring precise distance measurements and 3D mapping.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="operating-principle">Operating Principle<a href="#operating-principle" class="hash-link" aria-label="Direct link to Operating Principle" title="Direct link to Operating Principle" translate="no">​</a></h3>
<p>A LiDAR unit emits pulses of laser light and measures the time it takes for these pulses to return after hitting an object. By knowing the speed of light, it calculates the precise distance to that object. By rapidly scanning beams in multiple directions, a LiDAR sensor can generate a dense &quot;point cloud&quot; representing the 3D structure of its surroundings.</p>
<p>[Illustration: A diagram showing a LiDAR sensor emitting a laser pulse, the pulse reflecting off an object, and returning to the sensor. The concept of &quot;time-of-flight&quot; is highlighted.]</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="applications">Applications<a href="#applications" class="hash-link" aria-label="Direct link to Applications" title="Direct link to Applications" translate="no">​</a></h3>
<ul>
<li class=""><strong>Mapping and Localization</strong>: Creating detailed 3D maps of an environment and determining the robot&#x27;s position within them (SLAM).</li>
<li class=""><strong>Obstacle Detection</strong>: Identifying and avoiding obstacles with high precision, critical for autonomous navigation.</li>
<li class=""><strong>Object Recognition</strong>: Classifying objects based on their geometric shape and size from the point cloud data.</li>
</ul>
<p>[Example: An autonomous vehicle&#x27;s LiDAR point cloud showing the 3D geometry of a street scene, including other cars, pedestrians, and buildings.]</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="cameras-vision-systems">Cameras (Vision Systems)<a href="#cameras-vision-systems" class="hash-link" aria-label="Direct link to Cameras (Vision Systems)" title="Direct link to Cameras (Vision Systems)" translate="no">​</a></h2>
<p>Cameras are passive sensors that capture light from the environment to form an image, providing rich, dense information about the world. Vision is one of the most powerful sensing modalities for robots, enabling a wide range of intelligent behaviors.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="types-of-robotic-cameras">Types of Robotic Cameras<a href="#types-of-robotic-cameras" class="hash-link" aria-label="Direct link to Types of Robotic Cameras" title="Direct link to Types of Robotic Cameras" translate="no">​</a></h3>
<ol>
<li class=""><strong>Monocular Cameras</strong>: A single camera that provides a 2D projection of the 3D world. Inferring depth is challenging but can be estimated using cues and motion.</li>
<li class=""><strong>Stereo Cameras</strong>: Two cameras separated by a known distance, mimicking human binocular vision. By comparing the two images, depth can be calculated through triangulation.</li>
<li class=""><strong>RGB-D Cameras</strong>: These cameras provide a standard color (RGB) image plus a per-pixel depth (D) map. This is often achieved using structured light or time-of-flight principles.</li>
</ol>
<p>[Illustration: A comparison of outputs from a mono camera (2D image), a stereo camera (showing disparity), and an RGB-D camera (a color image and a corresponding depth map).]</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="core-concepts">Core Concepts<a href="#core-concepts" class="hash-link" aria-label="Direct link to Core Concepts" title="Direct link to Core Concepts" translate="no">​</a></h3>
<p>Robotic vision relies on processing pixel data to extract higher-level information. This includes identifying keypoints, detecting edges, segmenting objects from the background, and ultimately recognizing what those objects are.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="applications-1">Applications<a href="#applications-1" class="hash-link" aria-label="Direct link to Applications" title="Direct link to Applications" translate="no">​</a></h3>
<ul>
<li class=""><strong>Object Recognition and Tracking</strong>: Identifying and following specific objects or people.</li>
<li class=""><strong>Scene Understanding</strong>: Interpreting the broader context of an environment, such as identifying a room as a &quot;kitchen.&quot;</li>
<li class=""><strong>Visual Servoing</strong>: Using visual feedback to guide a robot&#x27;s motion, such as aligning a gripper with an object.</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="inertial-measurement-units-imus">Inertial Measurement Units (IMUs)<a href="#inertial-measurement-units-imus" class="hash-link" aria-label="Direct link to Inertial Measurement Units (IMUs)" title="Direct link to Inertial Measurement Units (IMUs)" translate="no">​</a></h2>
<p>An IMU is a proprioceptive sensor that measures a robot&#x27;s orientation, angular velocity, and linear acceleration. It is crucial for stabilizing a robot and tracking its motion, especially for dynamic systems like humanoids and drones.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="components-and-principles">Components and Principles<a href="#components-and-principles" class="hash-link" aria-label="Direct link to Components and Principles" title="Direct link to Components and Principles" translate="no">​</a></h3>
<p>An IMU typically combines two types of sensors:</p>
<ul>
<li class=""><strong>Accelerometers</strong>: Measure linear acceleration, including the constant pull of gravity.</li>
<li class=""><strong>Gyroscopes</strong>: Measure angular velocity or the rate of rotation.</li>
</ul>
<p>By integrating data from these components, an IMU can estimate the robot&#x27;s orientation (roll, pitch, yaw) relative to a starting position.</p>
<p>[Diagram: An exploded view of an IMU, showing the tiny MEMS structures of an accelerometer and a gyroscope.]</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="applications-2">Applications<a href="#applications-2" class="hash-link" aria-label="Direct link to Applications" title="Direct link to Applications" translate="no">​</a></h3>
<ul>
<li class=""><strong>State Estimation</strong>: Providing a high-frequency estimate of the robot&#x27;s orientation for balancing and stabilization.</li>
<li class=""><strong>Dead Reckoning</strong>: Estimating the robot&#x27;s position by integrating acceleration over time, though this is prone to drift.</li>
<li class=""><strong>Sensor Fusion</strong>: Fusing IMU data with other sensors (like cameras or GPS) to create a more accurate and robust estimate of the robot&#x27;s state.</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="forcetorque-sensors">Force/Torque Sensors<a href="#forcetorque-sensors" class="hash-link" aria-label="Direct link to Force/Torque Sensors" title="Direct link to Force/Torque Sensors" translate="no">​</a></h2>
<p>Force/Torque (F/T) sensors provide a robot with a sense of touch and interaction force. They measure the forces and torques applied to a robot&#x27;s structure, enabling delicate and controlled physical interaction with the environment.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="operating-principle-1">Operating Principle<a href="#operating-principle-1" class="hash-link" aria-label="Direct link to Operating Principle" title="Direct link to Operating Principle" translate="no">​</a></h3>
<p>F/T sensors are typically mounted in a robot&#x27;s wrists, joints, or fingertips. They are built with strain gauges that deform under load. This deformation changes their electrical resistance, which is measured and converted into force and torque values along multiple axes.</p>
<p>[Illustration: A cross-section of a multi-axis force/torque sensor, showing the placement of strain gauges to measure forces and torques in different directions.]</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="applications-in-robotics">Applications in Robotics<a href="#applications-in-robotics" class="hash-link" aria-label="Direct link to Applications in Robotics" title="Direct link to Applications in Robotics" translate="no">​</a></h3>
<ul>
<li class=""><strong>Force-Compliant Motion</strong>: Allowing a robot to &quot;feel&quot; its way through a task, such as inserting a peg into a hole.</li>
<li class=""><strong>Safe Human-Robot Interaction</strong>: Detecting unexpected contact and stopping or reducing force to ensure safety.</li>
<li class=""><strong>Grasping Control</strong>: Measuring grip force to hold an object securely without crushing it.</li>
</ul>
<p>[Example: A robotic arm uses a wrist-mounted F/T sensor to smoothly trace a surface while maintaining a constant contact force.]</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/bilalmk/ai-native-book/tree/main/docs/physical-ai/04-sensor-systems.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-native-book/docs/physical-ai/humanoid-robotics-landscape"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Humanoid Robotics Landscape</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li><li><a href="#the-role-of-perception" class="table-of-contents__link toc-highlight">The Role of Perception</a></li><li><a href="#lidar-light-detection-and-ranging" class="table-of-contents__link toc-highlight">LiDAR (Light Detection and Ranging)</a><ul><li><a href="#operating-principle" class="table-of-contents__link toc-highlight">Operating Principle</a></li><li><a href="#applications" class="table-of-contents__link toc-highlight">Applications</a></li></ul></li><li><a href="#cameras-vision-systems" class="table-of-contents__link toc-highlight">Cameras (Vision Systems)</a><ul><li><a href="#types-of-robotic-cameras" class="table-of-contents__link toc-highlight">Types of Robotic Cameras</a></li><li><a href="#core-concepts" class="table-of-contents__link toc-highlight">Core Concepts</a></li><li><a href="#applications-1" class="table-of-contents__link toc-highlight">Applications</a></li></ul></li><li><a href="#inertial-measurement-units-imus" class="table-of-contents__link toc-highlight">Inertial Measurement Units (IMUs)</a><ul><li><a href="#components-and-principles" class="table-of-contents__link toc-highlight">Components and Principles</a></li><li><a href="#applications-2" class="table-of-contents__link toc-highlight">Applications</a></li></ul></li><li><a href="#forcetorque-sensors" class="table-of-contents__link toc-highlight">Force/Torque Sensors</a><ul><li><a href="#operating-principle-1" class="table-of-contents__link toc-highlight">Operating Principle</a></li><li><a href="#applications-in-robotics" class="table-of-contents__link toc-highlight">Applications in Robotics</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ai-native-book/docs/intro">Tutorial</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ai-native-book/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>